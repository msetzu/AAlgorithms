\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}              % For \url{...}
\usepackage{amssymb}               % Math symbols
\usepackage{amsmath}               % Various commands like \text{...}
\usepackage{mathtools}             % Maths
\usepackage[noend]{algpseudocode}  % Pseudocode printer
\usepackage{listings}              % Source code printer
\usepackage{framed}                % To frame the computational cost
\usepackage{qtree}                 % Draw simple binary trees
\usepackage{amsthm}                % \proof environment
\newcommand*{\expect}{\mathsf{E}}  % Define statistical expected value
\newcommand*{\prob}{\mathsf{P}}    % Define statistical probability

\title{Advanced Algorithms Problems and Solutions}
\author{Mattia Setzu \and Giorgio Vinciguerra}
\date{October 2016}

\lstset{numbers=left} % Print line numbers in \begin{lstlisting}...

\begin{document}

\maketitle

\tableofcontents
\clearpage

\section{Hogwarts}

The Hogwarts School\footnote{\url{http://didawiki.cli.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/alg2/algo2_16/hogwarts.pdf}}
is modeled as a graph $G=(V, E)$ where $V$ is the set of castle's rooms and $E \subseteq V \times V$
is the set of the stairs.
Each stair is labelled with the time of appearance and disappearance, and can be
walked in both directions, therefore the graph is undirected.
The goal is to find, if possible, the minimum amount of time required to go from
the first to the last room.

\subsection{Solution 1: Preprocessing-then-Dijkstra}

Dijkstra is able to find the shortest path in a graph with non-negative weights
on its edges.
Our main idea is to create a Dijkstra compatible graph through a \textsc{normalize}
function, then apply Dijkstra to it in order to find the shortest path.
The core of the preprocessing is the \textsc{normalize} function which computes traversal
times between nodes at a given time \emph{time}:

\begin{algorithmic}[1]
  \Function{normalize}{$from$, $to$, $time$}:
    \State $t \gets \infty$
    \If{$start[v'] \leq t < end[v']$}  \Comment{No waiting time}
      \State $t \gets t + 1$
    \ElsIf{$t < start[v']$}            \Comment{Waiting time}
      \State $t \gets start[v'] + 1$
    \Else
      \State $t \gets \infty$          \Comment{Available time already expired}
    \EndIf
      \State \Return{$t$}
    \EndFunction
\end{algorithmic}

The normalize function is then applied to a node traversal:

\subsubsection{Pseudo-code}

\begin{algorithmic}[1]
  \State create vertex set $Q$ of unvisited nodes\;
  \State create vertexes set $E'$ of edges weight\;
  \State $time \gets 0$                   \Comment{Initial time for traversal}
  \State $edges \gets$ \Call{stairs\_of}{0}\;      \Comment{Get incoming/outgoing edges
   of the source node}
  \Function{process}{$node$, $time$}
    \If{$edge \in visited\_edges$}
      \State \Return{}
    \EndIf

    \State $traversal\_time \gets \infty$
    \ForAll{$neighbor \in neighbors\_of\_node$}
      \State $traversal\_time \gets$ \Call{traversal\_time}{$node$, $neighbor$, $time$}
      \State $E'[0][node] \gets traversal\_time$   \Comment{$E'[i][j]$ holds the
                                                        weight/traversal}
      \State \Comment{time for the stair between $i$ and $j$}

      \ForAll{$new\_neighbor \in neighbors\_of\_neighbor$}
        \State \Call{normalize}{$neighbor$, $new\_neighbor$, $traversal\_time$}
      \EndFor
    \EndFor
    \If{\Call{dijkstra}{$V, E'$} = $\infty$}
    	\State \Return{-1}
    \Else
      \State \Return{$t$}
    \EndIf
  \EndFunction
\end{algorithmic}

\begin{framed}
  \noindent
  \textbf{Computational cost}: $\Theta(n^{2})$ if the vertex set in \textsc{dijkstra} is implemented
  as an array. $O(|E|+|V|\log |V|)$ with Fibonacci heap.
\end{framed}

\subsection{Solution 2: HogwartsDijkstra}

\begin{algorithmic}[1]
  \Function{HogwartsDijkstra}{$G$}:
  \State create vertex set $Q$ of unvisited nodes
  \ForAll{vertex $v \in V$}      \Comment{initialization}
      \State $time[v] \gets \infty$  \Comment{unknown time from source to v}
      \State add $v$ to $Q$          \Comment{all nodes initially in Q}
  \EndFor
  \State $time[0] \gets 0$ \Comment{time from source to source}
  \While{$Q\ne \emptyset$}
      \State $u \gets x \in Q$ with $\min \{time[x]\}$
      \State remove $u$ from $Q$
      \ForAll{neighbor $v$ of $u$}:
          \If{$time[u] \leq appear[v]$}
              \State $alt \gets appear[v] + 1$ \Comment{wait the appearance of
                                                                    the stair}
          \ElsIf{$time[u] < disappear[v]$}
              \State $alt \gets time[u] + 1$       \Comment{use the stair}
          \Else
              \State $alt \gets \infty$            \Comment{the stair has
                                                        already disappeared}
          \EndIf
          \If{$alt < time[v]$}
              \State $time[v] \gets alt$           \Comment{a quicker path to
                                                            $v$ has been found}
          \EndIf
      \EndFor
  \EndWhile
  \State \Return{$time[|N|-1]$}
  \EndFunction
\end{algorithmic}

\begin{framed}
  \noindent
  \textbf{Computational cost}. See the previous section.
\end{framed}

\subsection{Solution 3: BFS-like traversal}

\begin{algorithmic}[1]
  \Function{reach}{$N$, $M$, $A[]$, $B[]$, $appear[]$, $disappear[]$}
      \For{$i=0$ to $M-1$}
          \State $edges\_[A[i]].push\_back(make\_pair(i, B[i]))$
          \State $edges\_[B[i]].push\_back(make\_pair(i, A[i]))$
      \EndFor
      \For{$i=0$ to $N-1$}
          \State $done\_[i] \gets false$
          \State $distance\_[i] \gets \infty$
      \EndFor
      \State $reached\_[0].push\_back(0)$
    \State $distance\_[0] \gets 0$
    \For{$t=0$ to $MAX\_TIME$}
      \ForAll{$v \in reached\_[t]$}
          \If{not $done\_[v]$}
          \ForAll{$edge \in edges\_[v]$}
            \State $staircase \gets edge.first$
            \State $neighbor \gets edge.second$
            \State $time \gets \max(distance_[v], appear[staircase])+1$
            \If{not $done\_[neighbor]$ \\ \hfill and $distance\_[v] < disappear[staircase]$ \\ \hfill and  $time < distance\_[neighbor]$}
              \State $distance\_[neighbor] \gets time$
              \State $reached\_[time].push\_back(neighbor)$
            \EndIf
          \EndFor
        \State $done\_[v] \gets true$
          \EndIf
      \EndFor
    \EndFor
    \State \Return{$(distance\_[N-1] = \infty) ? -1 : distance\_[N-1]$}
  \EndFunction
\end{algorithmic}

\begin{framed}
  \noindent
  \textbf{Computational cost}: $O(m + MAX\_TIME)$.
\end{framed}


\section{Paletta}

Paletta ordering\footnote{\url{http://didawiki.cli.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/alg2/algo2_16/paletta.pdf}}
is a peculiar ordering technique: given a 3-tuple of elements, paletta takes the
central element as pivot and swaps the two elements right before and next to it.
To make an example:
$$(3, 2, 1) \xrightarrow{paletta} (1, 2, 3)$$
We now want to develop an algorithm to order any array through paletta ordering
with the minimal number of swaps.
You should see as not every array can be ordered (e.g. [1, 3, 2]).

\subsection{Solution 1: Split and count-inversions}

We should note that the following properties hold:
\begin{enumerate}
    \item Every element can be a pivot, but the first and the last one, as they
    have respectively no elements before and after them.
    \item Every element can be swapped as many times as necessary, but only with
    elements of the same 2-remainder (numbers in even positions can only be
    swapped with numbers in even positions, the same holds for odd indexes).
    More formally, if $n$ is the size of the array $A$ we want to sort,
    $i,j \in [1, n - 2]$, $A[i]$ can be swapped with $A[j]$ if and only if $i \equiv j \pmod{2}$.
    \item The least number of swaps does not backtrack any element.
    Formally, let \emph{k} be the minimal number of swaps applied to an array,
    backtracks included. By hypothesis, \emph{k} is minimal, but at least \emph{m},
    $m > 0$ backtrack swaps have been operated, therefore we found a
    $k' = k - m: k' < k$, a new minimal number of swaps: contradiction.
\end{enumerate}

Given item 2, we can split our array in two, even and odd numbers, and order
them counting the swaps.
In our example we'll use \emph{mergesort}, as it runs in $O(n\log n)$, does
backtrack elements, and is very well-known.
Clearly, given an array, a swap happens when an element is pushed back, pulling
the one between its new position and the old one ahead: we can map this behaviour
in the merge routine of mergesort: the array merged is able to push back elements
from its right pointer to the new array, moving them back of $(m - i) + (j - m)$ positions,
where \emph{m} is the dimension of the current two sub-arrays to merge.
Provided that our edited version of mergesort ran successfully on both the
even-index and odd-index, we now need to verify if by merging them we obtain an
ordered array.
Intuitively, the merged array will start with the first element of the even-index
arrays, followed by the first of the odd-index array, followed by the second of
the even-index array, and so on.
To check for these elements is pretty trivial and can be done in linear time.
Follows the pseudo-code for the edited version and \textsc{snake\_check} function:

\begin{algorithmic}[1]
  \Function{merge\_with\_paletta}{$left$, $right$, $k$}:
    \State ...                        \Comment{merge instructions}
    \If{$right > left$}
      \State $paletta\_count \gets paletta\_count + 1$
      \State ...
      \EndIf
    \EndFunction
\end{algorithmic}

\begin{algorithmic}[1]
  \Function{snake\_check}{}
    \State $even, odd \gets 0$
    \For{;$even, odd < N;even = even + 1, odd = odd + 1$}
      \If{$a[even] > a[odd]$}
        \State \Return{$-1$}\;
      \EndIf
    \EndFor
    \State \Return{$paletta\_count$}\;
    \EndFunction
\end{algorithmic}

\begin{framed}
  \noindent
  \textbf{Computational cost}: $\Theta(n\log n)$.
\end{framed}


\newpage
\section{Range updates}

Consider an array \emph{C} of n integers, initially all equal to zero.
We want to support the following operations:
\begin{itemize}
    \item \textbf{update(i, j, c):} where $0 \leq i \leq j \leq n - 1$ and $c$ is
    an integer: it changes $C$ such that $C[k] = C[k] + c$ for every $i \leq k \leq j$.
    \item \textbf{query(i)} where $0 \leq i \leq n - 1$: it returns the value of $C[i]$.
    \item \textbf{sum(i,j)} where $0 \leq i \leq j \leq n - 1$: it returns
                    $\Sigma_{k = 1}^{j}(C[k])$.
\end{itemize}

Design a data structure that uses $O(n)$ space and implements each operation above
in $O(\log(n))$ time. Note that $query(i) = sum(i, i)$ but it helps to reason.
[Hint to further save space: use an implicit tree such as the Fenwick tree (see wikipedia).]

\subsection{Solution 1: Fenwick lazy a-b sums}
Let $T$ be a segmented binary tree over a continuous interval $I: [0, N - 1]$
s.t.\ its leafs are the points in I, and the parent of two nodes comprises of their interval:
$$  n' \cup n'' = n, n' \cap n'' = \emptyset   \textrm{ s.t. } n \text{ is the parent of } n', n''$$

$T$ will keep track of the prefix sums for every interval.
We define a function
\begin{equation}
    s': [0, n - 1] \to \mathbb{N}
\end{equation}
that given a node in $T$ returns the value associated with $I$, namely the
cumulative sum of that interval.

In order to reduce the computational cost, we introduce a lazy algorithm
that doesn't propagate sums over $T$ as they are streamed in the input,
which means $s'(i)$ might not be accurate at a given time $t$ for any of the
requested operation.

We'll instead either compute over $T$ or update $T$ as necessary.
Let us define a function to do so:
\begin{equation}
    l: \mathbb{N} \to (\mathbb{N} \cup \{\epsilon\}, \mathbb{N})
\end{equation}
to keep track of our lazy sums:
\begin{equation*}
    s(n) = \begin{cases}
            \epsilon, \_            &   \textrm{if no lazy prefix sum is in that interval} \\
            k, m                    &   \textrm{if a lazy sum of k is to be propagated to m}\\
            \end{cases}
\end{equation*}
The \textsc{query} function is then trivial:

\begin{algorithmic}[1]
  \Function{query}{$I$, $i$, $sum$}:
    \If{$I.size = 1$}                       \Comment{Return found value}
      \State \Return $I.sum$
    \EndIf
    \If{lazy(I), $i \in I.left, i \notin I.right$}     \Comment{Lazy on
                                                                    left child}
      \State $lazy(I) \gets False$
      \State \Call{query}{$I.left$, $i$, $sum + I.sum$}
    \EndIf

    \If{lazy(I), $i \in I.right, i \notin I.left$} \Comment{Lazy on
                                                                    right child}
      \State $lazy(I) \gets False$
      \State \Call{query}{$I.right$, $i$, $sum + I.sum$}
    \EndIf

    \If{lazy(I), $i \in I.right, i \in I.left$}      \Comment{Lazy on both}
      \State $lazy(I) \gets False$
      \State \Call{query}{$I.right$, $i$, $j$, $sum + I.sum$} +
                \Call{query}{$I.left$, $i$, $j$, $sum + I.sum$}
    \EndIf

    \If{!lazy(I), $i \in I.left$}             \Comment{Not lazy on left
                                                                        child}
      \State \Call{query}{$I.left$, $i$, $sum$}
    \EndIf

    \If{!lazy(I), $i \in I.right$}        \Comment{Not lazy on right child}
      \State \Call{query}{$I.right$, $i$, $sum$}
    \EndIf

    \If{!lazy(I), $i \in I.right, i \in I.left$}         \Comment{Not lazy
                                                                        on both}
      \State \Call{sum}{$I.right$, $i$, $sum$}
    \EndIf
    \EndFunction
\end{algorithmic}

\begin{algorithmic}[1]
  \Function{sum}{$I$, $i$, $j$, $sum$}:
    \If{$I.size = 1$}                                   \Comment{Return
                                                                    found value}
      \State \Return $I.sum + sum$\;
    \EndIf

    \If{lazy(I), $i \in I.left, i \notin I.right$}     \Comment{Lazy on
                                                                    left child}
      \State $lazy(I) \gets False$
      \State \Call{sum}{$I.left$, $i$, $j$, $sum + I.sum$}
    \EndIf
    \If{lazy(I), $i \in I.right, i \notin I.left$} \Comment{Lazy on
                                                                    right child}
      \State $lazy(I) \gets False$
      \State \Call{sum}{$I.right$, $i$, $j$, $sum + I.sum$}
    \EndIf

    \If{lazy(I), $i \in I.right, i \in I.left$}      \Comment{Lazy on
                                                                        both}
      \State $lazy(I) \gets False$
      \State \Call{sum}{$I.right$, $i$, $j$, $sum + I.sum$} +
                \Call{sum}{$I.left$, $i$, $j$, $sum + I.sum$}
    \EndIf
    \If{!lazy(I), $i \in I.left$}                        \Comment{Not lazy
                                                                    on left child}
      \State \Call{sum}{$I.left$, $i$, $sum$}
    \EndIf
    \If{!lazy(I), $i \in I.right$}                       \Comment{Not lazy
                                                                    on right child}
      \State \Call{sum}{$I.right$, $i$, $sum$}
    \EndIf

    \If{!lazy(I), $i \in I.right, i \in I.left$}         \Comment{Not lazy
                                                                        on both}
      \State \Call{sum}{$I.right$, $i$, $sum$}
    \EndIf
    \EndFunction
\end{algorithmic}

\begin{algorithmic}[1]
  \Function{update}{$I$, $i$, $j$, $k$}:
    \If{$I.size = 1$}                                   \Comment{Return
                                                                    found value}
        \State \Return $I.val \gets I.val + update$\;
    \EndIf
    \If{lazy(I), $i \in I.left, i \notin I.right$}     \Comment{Lazy on
                                                                    left child}
        \State $lazy(I.left) \gets True$
        \State $I.left.val \gets k$
    \EndIf
    \If{lazy(I), $i \in I.right, i \notin I.left$} \Comment{Lazy on right
                                                                            child}
        \State $lazy(I.right) \gets True$
        \State $I.right.val \gets k$
    \EndIf
    \If{lazy(I), $i \in I.right, i \in I.left$}      \Comment{Lazy on both}
        \State $lazy(I) \gets True$
        \State $I.val \gets k$
    \EndIf
    \If{!lazy(I), $i \in I.left$}                        \Comment{Not lazy
                                                                    on left child}
        \State \Call{update}{$I.left$, $i$, $update$}
    \EndIf
    \If{!lazy(I), $i \in I.right$}                       \Comment{Not lazy
                                                                on right child}
        \State \Call{update}{$I.right$, $i$, $update$}
    \EndIf

    \State \If{!lazy(I), $i \in I.right, i \in I.left$}         \Comment{Not lazy
                                                                        on both}
        \State \Call{update}{$I.right$, $i$, $update$}
    \EndIf
    \EndFunction
\end{algorithmic}


\newpage
\section{Karp-Rabin}
Given a string $S: |S| = n$, and two positions $0 \leq i < j \leq (n - 1)$,
the longest common extension $lceS(i, j)$ is the length of the maximal run of matching
characters from those positions, namely: if $S[i] 6= S[j]$ then $lceS(i, j) = 0$;
otherwise, $lceS(i, j) = \max{l \geq 1 : S[i ... i + l - 1] = S[j ... j + i - 1]}$.
For example, if S = abracadabra, then $lceS(1, 2) = 0$, $lceS(0, 3) = 1$, and
$lceS(0, 7) = 4$.
Given S in advance for preprocessing, build a data structure for S based
on the Karp-Rabin fingerprinting, in $O(n \ln(n)$) time, so that it supports subsequent
online queries of the following two types:
\begin{itemize}
    \item $lceS(i, j)$: it computes the longest common extension at positions i
    and j in O(log n) time.
    \item $equals (i, j, c)$: it checks if $S[i ... i + ` - 1] = S[j ... j + ` - 1]$ in constant time.
\end{itemize}
Analyze the cost and the error probability.
The space occupied by the data structure can be $O(n \log(n))$ but it is possible
to use $O(n)$ space.
[Note: in this exercise, a onetime preprocessing is performed, and then many online
queries are to be answered on the fly.]

\subsection{Solution 1: Cumulative shift}
\subsubsection{Construction}

In order to save computational cycles on checks over ranges we use a similar structure
to the one in the range updates: we compute the hashing on the first character in $O(1)$
time, then roll the hash through the $n - 1$ remaining characters through $n O(1)$
operations.
We call $H$ this array; we also denote $h_k$ as the function $c a^{i}$ computating
the Rabin-Karph hash of a string $s$.
The reader shall now see that $\exists h^{-1}(s)$: that is, $h$ is invertible in $O(1)$.
The entries $h[i] = \sum_{i \in [0, n - 1]}(h(i))$ have cumulative hash and the following
properties hold:
    \begin{itemize}
    \item $h[s[i]] = (h[i] - h[i - 1]) / a^{-1}, a^{-1} = a^{1}$
    \item $h[i..j] - h[k..l] = (h[l] - h[k - 1]) / a^{-1} -
            (h[j] - h[i - 1]) / a^{-1}, a^{-1} = \textrm{modular inverse}$
    \end{itemize}

\subsubsection{equals(i, j, l)}

\textsc{equals} works on cumulative hashes, subtracting them and scaling them
accordingly, as our $rabin$ function multiplies by an $a^{i}$ costant.

\begin{algorithmic}[1]
  \Function{equals}{$i$, $j$, $length$}:
    \State $h_i = h[i + length] - h[i - 1]$\;
    \State $h_j = h[j + length] - h[j - 1]$\;

    \State $h^{i} = h_i / inv(a, i, l)$\;
    \State $h^{j} = h_j / inv(a, j, l)$\;

    \Return{$h^{i} - h^{j} == 0$}\;
    \EndFunction

    \Function{inv}{$h$, $k$, $l$}:
    \Return $h^{k - l}$
    \EndFunction
\end{algorithmic}

\subsubsection{lce(i, j)}

\textsc{lce} works on cumulative hashes, checks the equality on the middle element
of the strings and runs recursively on the half with different hashing.
We define \textsc{lce} as an auxiliary function

\begin{algorithmic}[1]
  \Function{lce}{$i$, $j$, $l$}:
    \State eq = \Call{equals}{$i$, $j$}\;

    \If{eq}
        \Return{$l$}
    \ElsIf{$\neg$ \Call{equals}{$(j - i) / 2$, $(n - j) / 2$, $l$}}
        \Return{\Call{equals}{$(j - i) / 2$, $(n - j) / 2$}}                            % First half
        \Else $(j - i) / 2 + $ \Return{\Call{equals}{$(j - i) / 2$, $(n - j) / 2$}}     % Second half
    \EndIf

    \State $h_i = h[i + length] - h[i - 1]$\;
    \State $h_j = h[j + length] - h[j - 1]$\;

    \State $h^{i} = h_i / inv(a, i, l)$\;
    \State $h^{j} = h_j / inv(a, j, l)$\;

    \Return{$h^{i} - h^{j} == 0$}\;
    \EndFunction

    \Function{inv}{$h$, $k$, $l$}:
    \Return $h^{k - l}$
    \EndFunction
\end{algorithmic}

\newpage
\section{Depth of a node in a random search tree}

A random search tree for a set S can be defined as follows: if $S$ is empty, then
the null tree is a random search tree; otherwise, choose uniformly at random a key
$k \in S$: the random search tree is obtained by picking
$k$ as root, and the random search trees on $L = \{x \in S : x < k\}$ and $R = \{x \in S :
x > k\}$ become, respectively, the left and right subtree of the root $k$.
Consider the randomized QuickSort discussed in class and analyzed with indicator
variables \href{http://didawiki.cli.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/alg2/algo2_13/randqs.pdf}{CLRS 7.3},
and observe that the random selection of the pivots follows the above process,
thus producing a random search tree of n nodes. Using a variation of the analysis
with indicator variables, prove that:
\begin{enumerate}
  \item the expected depth of a node (i.e.\ the random variable representing the distance of the node from the root) is nearly $2 \ln n$;
  \item the expected size of its subtree is nearly $2 \ln n$ too, observing that it is a simple variation of the previous analysis;
  \item the probability that the expected depth of a node exceeds $2\ln n$ is small for any given constant $c > 1$.
\end{enumerate}

\subsection{Recursive balanced proof}

Let $n$ be the number of nodes in the input list $l$, $h = \log_2(n)$ the height
of a balanced tree over $l$, $T(p)$ the tree built over the permutation $p$ of pivots,
$d(m)$ be the positional distance of a value $m$ of a partition from the median
value of the said partition.
Then the following holds:

    \begin{itemize}
    \item $height(T) = h \iff |T.left| = |T.right| \pm 1$ Trivially, let $r$ be
    the root  of a 3-nodes partition: then, if the partition is unbalanced, the
    lesser one will comprise of 0 nodes, while the greater one of 2, which implies
    that $height(T.right) == 2$.
    \label{k_distance} \item P = pivot,
    $d(m) = \pm k \implies height(T.left) = height(T.right) \pm k$.
    Recursively from the previous statement, a partition unbalanced of one element
    generates subtrees whose levels differ on a factor of $1$.
    By iterating recursively, their subtrees, if unbalanced by $1$, will yield
    one more level difference.
    Over $k$ unbalanced pivots on a single subtree, at most $k$ levels will be
    added to $h$.
    \item By the previous statement, it follows that $\nexists T, T': height(T) >= height(T')$,
    T balanced, T' unbalanced.
    As stated, let $T', T$ be the unbalanced/balanced tree respectively; let us
    cheat with $T$ and switch the root pivot with the first element in its subtree.
    Now, let us prove by contradiction that $T$ can't stay balanced and that its
    height will increase.
    By shifting the tree to the left we have deprived $T.right$ of either 0 levels
    (in case $T.right$ is able to switch every pivot in its tree with its right
    subtree root, ending with the rightmost leaf in its subtree) or 1, in case no
    rightmost leaf is present.
    Therefore $height(T) <= height(T')$.
    \item The completely unbalanced tree is the tree with the most levels.
    By taking partitions of size 0 we costantly force, at each level, one subtree
    to disappear.
    Therefore, its level(s) has to be necessarly transferred to its brother.
    We then have exactly one node per level, therefore $n$ levels.
    \end{itemize}

\subparagraph{Behaviour on random permutations}
Now let us analyse how the tree depth varies according to random pivot selection.
We start by applying the~\ref{k_distance}k-distance to a tree $T$ with $n = 3$
nodes.
Trivially, $height(T)$ with balanced tree is equal to two.
Now, let us pick either the lowest or the greatest pivot possible: the tree
is unbalanced towards either the left or the right, but $height(T) = 2$ in
both cases.
As the reader can see from~\ref{k_distance}, the distance works in absolute
value; it is then clear how, at every permutation for a pivot $p$, out of the
$n$, there are $2$ that generate a tree of the same height:
$p = d(P) + k, p = d(P) - k$.
Given that at every iteration a node $x$ in a completely unbalanced tree $T$'
has a probability of $\frac{1}{n - i}$, we can define the probability of $x$
being a pivot at level $l$ as:
    \begin{equation}
    P(x_{k}) = \frac{1}{n - l}
    \end{equation}
Now, in order for $x$ not to be chosen as pivot in the previous $l - 1$
levels we have:
    \begin{equation}
    P(x_{k}) = \Sigma_{k = 1}^{l - 1} (\frac{1}{n - l + 1})
    \end{equation}
Given the height of $T$, the (harmonic) partial series converges to
$\ln{(n)} + 1$.
Let us now add a root $r$ s.t. $T'.right = T, T'.left = T$.
We now have to consider the mirror case $\ln{(n')} + \ln{(n')}$,
given by the previous $n' = n/2$ in the logarithm, since the number of nodes
doubled, the $+1$ removed for both, since now neither of $T'.left, T.right$
is the root, and a $+1$ added since a new level has been added.

\subsection{Upper bound}

By hypothesis,
    \begin{equation}
    E[d(x) > 2 c \ln(n)] <<< 1
    \end{equation}
By definition the ancestor of a node $i$ are indipendent random variables,
and we can apply the \href{https://en.wikipedia.org/wiki/Chernoff_bound}{Chernoff
bounds} over the set ${x: d(x) >= 2 \ln(n)}$ of random variables determining
the expected distance of nodes.
\begin{equation*}
    \prob[X \geq c \expect[X]] < e^{-c \ln(\frac{c}{e})\expect[X]}
\end{equation*}
Let us consider $X = 1 \forall i == \ln(n)$, the expected depth of $\ln(n)$,
then
\begin{equation*}
    \prob[X \geq c \ln(n)] < e^{-c \ln(\frac{c}{e})\ln(n)}
\end{equation*}

\subsection{Proof with indicator variable}

\textbf{Prove that the expected depth of a node is nearly $2 \ln n$.}

\begin{proof}
  Let $z_m$ the $m$th smallest element in $S$ and $$X_{ij}=
  \left\{
    \begin{array}{ll}
      1 & \mbox{if $z_j$ is an ancestor of $z_i$ in the random search tree} \\
      0 & \mbox{otherwise}
    \end{array}
  \right.$$
  The depth of the node $i$ in the tree is given by the number of its ancestors:
  \begin{equation}
    X=\sum_{\substack{j=1 \\ j\ne i}}^n X_{ij}
    \label{equation:node-depth}
  \end{equation}

  Note that the depth of a node is also equal to the number of comparison it's
  involved in (in other words, the number of times it became the left or the
  right child of a randomly chosen pivot).

  Once a pivot $k$ is chosen from $S$, $S$ is partitioned in two subsets $L$ and
  $R$. The elements in the set $L$ will not be compared with the elements in $R$
  at any subsequent time. The event $E_1=$``$z_j$ is an ancestor of $z_i$ in the
  random search tree" occurs if $z_j$ and $z_i$ belongs to the same partition
  \emph{and} $z_j$ was chosen as pivot before $z_i$. The probability that $E_1$
  occurs, since it is the intersection of two events, can be upper bounded by:
  $$\text{Pr}\{ z_j \text{ was chosen as pivot before } z_i
  \}=\frac{1}{\text{size of the partition}}\leq\frac{1}{|j-i|+1}$$ because
  pivots are chosen randomly and independently, and because the partition that
  contains both $z_j$ and $z_i$ must contain \emph{at least} the $|j-i|+1$
  numbers between $z_j$ and $z_i$.

  Taking expectations of both sides of \eqref{equation:node-depth}, and then
  using linearity of expectation, we have:
  \begin{align*}
    E[X] & = \sum_{\substack{j=1\\ j\ne i}}^n E[X_{ij}] \\
    & = \sum_{\substack{j=1\\ j\ne i}}^n \text{Pr}\{ z_j \text{ is an ancestor
      of } z_i \text{ in the random search tree} \} \\
    & \le \sum_{\substack{j=1\\ j\ne i}}^n \frac{1}{|j-i|} \\
    & = \sum_{j=1}^{i-1} \frac{1}{i-j} + \sum_{j=i+1}^{n} \frac{1}{j-i}
  \end{align*}
  With the change of variables $l=i-j$ and $m=j-i$:
  \begin{equation*}
    = \sum_{l=1}^{i-1} \frac{1}{l} + \sum_{m=1}^{n} \frac{1}{m} \approx 2\ln n
  \end{equation*}

\end{proof}

\textbf{Prove that the expected size of its subtree is nearly $2 \ln n$ too,
observing that it is a simple variation of the previous analysis.}

\begin{proof}
  The size of the subtree of a randomly chosen pivot of $z_j\in S$ is given by
  the number of it's descendants. Since \eqref{equation:node-depth} is the
  number of ancestors of a node $z_i$, we can find the number of descendants of
  $z_j$ by changing the summation from $j=1,\dotsc,n$ to $i=1,\dotsc,n$.
\end{proof}

\newpage
\section{Hashing sets}

Your company has a database $S \subseteq U$ of keys. For this database, it uses
a randomly chosen hash function $h$ from a universal family $H$ (as seen in class);
it also keeps a bit vector $B_S$ of $m$ entries, initialized to zeroes, which are
then set $B_S[h(k)] = 1 \forall k \in S$ (note that collisions may happen).
Unfortunately, the database has been lost, thus only BS and h are known, and the
rest is no more accessible.
Now, given $k \in U$, how can you establish if $k$ was in $S$ or not?
What is the probability of error? (Optional: can you estimate the size $|S|$ of
$S$ looking at h and $B_S$ and what is the probability of error?)
Later, another database R has been found to be lost: it was using the same hash
function h, and the bit vector BR defined analogously as above.
Using $h, B_S, B_R$, how can you establish if $k$ was in $S \cap R$ (union), $S \cup R$
(intersection), or $S \\ R$ (difference)? What is the probability of error?

\subsection{$k \in S$}
\label{k_in_s}

Let $k \in [0, m - 1]: B_s[k] = 1 \implies h(k) \in S$, but not conclusive, as
$\exists h \in H_{a,b}, l \in U: h(l) = k$.
Out of the $|H_{a, b}| = (p)(p - 1)$ functions $\in H_{a, b}$ the probability
    \begin{equation}
    \prob(h_{a_{i},b_{i}}(l) = h_{a_{0},b_{0}}(k))
    \end{equation}

\subsection{$|S|$}
\label{size_of_S}

Since $h$ is bijective, and we know $h$, we can identify a lower bound, but are
not able to estimate correctly the exact number.
Trivially, we count the insertions in $B_S$ with a $\phi(B_S)$, and we'll have
$|S| \geq \phi(B_S)$.
Since we have no access to the collision list, we dont know how many collisions
there are on every $B_S$ entry to 1.

\subsection{Set operations}

\begin{itemize}
    \item \textbf{Union} Trivially proven by~\ref{k_in_s} applied to $R, S$.
    If any of them $\in [m]$, then $k \in S \cup R$.
    \item \textbf{Intersection} Trivially proven by~\ref{k_in_s} applied to $R, S$.
    If both $\in [m]$, then $k \in S \cap R$.
    \item \textbf{Difference} Trivially proven by complement on union.
\end{itemize}


\newpage
\section{Family of uniform hash functions}

The notion of pairwise independence says that, for any $x_{1} \neq x_{2}, c_{1}, c_{2}
\in \mathbb{Z}_{p}$, we have that
    \begin{equation}
    \prob(h(x_{1} = c_{1}), h(x_{2} = c_{2})) = \prob(h(x_{1} = c_{1})) * \prob(h(x_{2} = c_{2}))
    \end{equation}
In other words, the joint probability is the product of the two individual probabilities.
Show that the family of hash functions $H = {h_{a,b}(x) = ((ax + b) \mod p) \mod m}:
a \in \mathbb{Z}^{*}_{p},
b \in \mathbb{Z}^{*}_{p}$ is \emph{pairwise dependent}
where $p$ is a sufficently large prime number ($m + 1 \leq p \leq 2m$).

\subsection{Equal probability}

By linear algebra, $ak \mod p = c \forall k \in \mathbb{N}$.
If we were to cap $ak \mod p = c \forall k \in K, K_{N} = {k_{i}: k_{i} < N}$
\begin{equation}
\prob(h(x_{i} = c_{i})) = \frac{1}{m^{2}} = \prob(h(x_{1} = c_{1}), h(x_{2} = c_{2}))
\end{equation}
Given $x_{i}$ we define as $m_{i} = (a x_{i} + b) \mod p$; since $(a x_{i} + b)$
is a \emph{linear transformation} $m_{i}$ is unique.
It follows trivially that $\prob((a x_{i} + b) = d) = \frac{1}{p}$.
Now, the same goes for $x_{1}, x_{2}$:
    \begin{gather}
    (a x_{1} + b) \equiv d                                                           \\
    (a x_{2} + b) \equiv e
    \end{gather}
By the \href{http://en.wikipedia.org/wiki/Chinese_remainder_theorem}{Chinese reminder theorem}
the above system has only one solution, therefore $\prob((a x_{1} + b) = d) = \frac{1}{p}$ and
$e$ become independent of $d$.
Having
\begin{equation}
    \prob(h(x_{1} = c_{1}), h(x_{2} = c_{2})) =
    \prob(h(x_{1} = c_{1})) * \prob(h(x_{2} = c_{2})) =\frac{1}{p} * \frac{1}{p} = \frac{1}{p^{2}}
\end{equation}
Which, since $m + 1 \leq p \leq 2m$, proves the assumption.
\end{document}
